# ==============================
# Project Metadata
# ==============================
project:
  name: "Intelligent Demand Sensing & Autonomous Inventory Planning"
  short_name: "IDS"
  version: "1.0.0"
  environment: "local"
  timezone: "Asia/Kolkata"


# ==============================
# Paths
# ==============================
paths:
  data:
    raw: "data/raw"
    processed: "data/processed"
    calendar: "data/calendar"
    external: "data/external"

  output:
    forecasts: "output/forecasts"
    inventory: "output/inventory"
    metrics: "output/metrics"
    plots: "output/plots"
    reports: "output/reports"
    model: "output/model"

  logs: "logs"


# ==============================
# Logging & Reproducibility
# ==============================
logging:
  level: "INFO"
  log_to_file: true
  filename: "ids_project.log"

seeds:
  global_seed: 42
  propagate_to_models: true


# ==============================
# Data Schema & Normalization
# ==============================
data_schema:

  # -------------------------
  # Sales dataset schema
  # -------------------------
  sales:
    date_column: "date"
    target_column: "total_sales"

    required_columns:
      - date
      - total_sales
      - sales_open
      - sales_closed

    optional_columns:
      - is_festival
      - is_holiday
      - is_promo
      - weekday_weekend
      - day

    optional_column_policy:
      on_missing: "warn"          # warn | ignore | error

    normalize_columns: true     # Convert to lowercase and strip spaces

    rename_map:                  # Standardize column names across datasets
      date: ["date"]
      total_sales: ["total_sales", "sales", "total"]
      sales_open: ["sales_open", "open_sales"]
      sales_closed: ["sales_closed", "closed_sales"]

# -------------------------
  # Calendar dataset schema
  # -------------------------
  calendar:
    date_column: "date"

    required_columns:
      - date
      - is_festival
      - is_holiday

    optional_columns:
      - festival_name
      - day
      - weekday_weekend

    optional_column_policy:
      on_missing: "warn"

    normalize_columns: true

    rename_map:
      date: ["date"]
      is_festival: ["festival", "festival_flag", "is_festival"]
      is_holiday: ["holiday", "is_holiday"]
      festival_name: ["festival_name", "fest_name"]
      is_promo: ["promo", "promotion", "is_promo"]
      weekday_weekend: ["weekday_weekend", "week_type"]
      day: ["day", "weekday", "day_of_week"]

# ==============================
# Ingestion Configuration
# ==============================
ingestion:

  # ------------------------------------------------
  # Demand anchor dataset (MANDATORY)
  # ------------------------------------------------
  # This dataset must:
  # - Be enabled
  # - Contain the target_column (e.g., total_sales)
  # - Act as the primary demand signal for the pipeline
  demand_source: "sales"

  # ------------------------------------------------
  # Global ingestion behavior (applies to ALL sources)
  # ------------------------------------------------
  header: true
  skip_rows: 0

  na_values:
    - ""
    - "NA"
    - "N/A"
    - "null"
    - "None"

  strict_load: true   # Fail entire pipeline if any dataset fails


  # =================================================
  # INPUT DATASETS (MULTIPLE, AUTOMATIC)
  # =================================================
  datasets:

    # ------------------------------------------------
    # 1. CSV
    # ------------------------------------------------
    calendar:
      enabled: true
      source_type: csv

      # File is resolved as:
      # paths.data.raw + file
      file: "calendar_holidays.csv"

      csv:
        delimiter: ","
        encoding: "utf-8"


    # ------------------------------------------------
    # 2. EXCEL (PRIMARY DEMAND SOURCE)
    # ------------------------------------------------
    sales:
      enabled: true
      source_type: excel

      file: "sales_data.xlsx"

      excel:
        # OPTION 1: Load a single sheet
         sheet_name: "22-24"

        # OPTION 2: Load selected multiple sheets (ACTIVE)
       # sheet_names:
        #  - "Calendar"
        #  - "Holidays"
        #  - "Sales"

        # OPTION 3: Load ALL sheets
        # load_all_sheets: true


    # ------------------------------------------------
    # 3. PARQUET
    # ------------------------------------------------
    demand_history:
      enabled: false
      source_type: parquet

      file: "demand_history.parquet"

      parquet:
        engine: "pyarrow"


    # ------------------------------------------------
    # 4. DATABASE
    # ------------------------------------------------
    master_data:
      enabled: false
      source_type: database

      database:
        engine: "postgresql"
        host: "localhost"
        port: 5432
        database: "inventory_db"
        schema: "public"

        table: "product_master"

        username_env: "DB_USER"
        password_env: "DB_PASS"

        fetch_size: 50000


    # ------------------------------------------------
    # 5. API
    # ------------------------------------------------
    promotions:
      enabled: false
      source_type: api

      api:
        method: "GET"
        url: "https://api.example.com/promotions"

        headers:
          Authorization: "Bearer ${API_TOKEN}"
          Content-Type: "application/json"

        params:
          region: "IN"
          active: true

        timeout_seconds: 30


        # Pagination (OPTIONAL)
        # pagination:
        #   enabled: true
        #   page_param: "page"
        #   size_param: "limit"
        #   page_size: 1000


# ==============================
# Feature Engineering
# ==============================
features:
  lag_features:
    enabled: true
    lags: [1, 7, 14, 30]

  rolling_features:
    enabled: true
    windows: [7, 14, 30]

  calendar_features:
    enabled: true
    day_of_week: true
    is_weekend: true
    month: true

  include_promotions: false

  demand_bucketing:
    method: "quantile"
    labels: ["Low", "Medium", "High"]


# ==============================
# Modeling (Base Training Params)
# ==============================
modeling:
  train_test_split:
    method: "time"
    test_days: 90

  forecast_horizon_days: 365

  models:
    linear_regression:
      enabled: true

    random_forest:
      enabled: true
      params:
        n_estimators: 200
        max_depth: 10
        
    xgboost:
      enabled: true
      params:
        learning_rate: 0.1
        n_estimators: 150
        max_depth: 8
       

    lightgbm:
      enabled: true
      params:
        learning_rate: 0.05
        n_estimators: 200
        max_depth: -1
       
    prophet:
      enabled: true
      yearly_seasonality: true
      weekly_seasonality: true


# ==============================
# Hyperparameter Tuning (Optional)
# ==============================
hyperparameter_tuning:
  enabled: false
  only_for_enabled_models: true

  random_forest:
    n_estimators: [100, 200, 300]
    max_depth: [5, 10, 20]

  xgboost:
    learning_rate: [0.05, 0.1]
    max_depth: [6, 8]


# ==============================
# Ensemble
# ==============================
ensemble:
  method: "stacking"
  meta_model: "linear_regression"
  use_tuned_models: true


# ==============================
# Inventory Planning
# ==============================
inventory:
  service_level: 0.95
  forecast_consumption_window_days: 7
  lead_time_days: 7
  demand_variability_method: "std"


# ==============================
# LLM What-if Agent
# ==============================
llm:
  enabled: true
  runtime_control:                    # Protects: Cost, Compliance, Reliability, Release safety
    allow_in_production: false
    allow_in_training: true
    allow_in_ci: false
  
  model_name: "gpt-4"
  temperature: 0.2
  max_tokens: 500
  fallback_regex: true

  guardrails:
    max_demand_change_pct: 50
    max_lead_time_multiplier: 3


# ==============================
# Execution Controls
# ==============================
execution:
  mode: "train"                      # dev | train | prod | backfill
  save_intermediate_outputs: true    # true| true| false| true
  overwrite_existing_outputs: false  # false| false| false| true
