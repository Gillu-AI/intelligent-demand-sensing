Detailed Explanation of Every Folder & File (Senior-level, Fresher-friendly)

Iâ€™ll explain this exactly in the style you asked, similar to your examples.

ğŸ“ config/

Purpose: Central brain of the project

config.yaml

Used to store all configurable values such as:

Data paths

Model parameters

Feature flags

Forecast horizon

Inventory CSL

LLM settings

Why it exists:

Avoids hardcoding

Easy experimentation

Business + ML separation

This file controls WHAT the system does, not HOW.

ğŸ“ data/

Purpose: Stores all data given by or generated for the project

This folder represents customer and system data, not code.

ğŸ“ data/raw/

Stores original input data from business/customer

Examples:

CSV

Excel

â— This data is never modified directly

Acts as a legal & audit reference

Example: Sales data from 2022â€“2024

ğŸ“ data/calendar/

Stores holiday, festival, and event data

Used to capture seasonality effects

Joined during feature engineering

ğŸ“ data/processed/

Stores cleaned and transformed datasets

Output of preprocessing & feature engineering

Used as input for modeling

Think of this as the â€œready-for-MLâ€ data layer

ğŸ“ outputs/

Purpose: Final deliverables for business & stakeholders

ğŸ“ outputs/forecasts/

Stores forecasted demand values (CSV)

Example: 2025 daily demand forecast

ğŸ“ outputs/plots/

Stores graphs and visualizations

Used in presentations and reports

ğŸ“ outputs/reports/

Stores evaluation summaries

Business insights

Observations & recommendations

ğŸ“ logs/

Purpose: Debugging, monitoring, audit

Contains execution logs

Critical for:

Production readiness

Pharma/enterprise compliance

Debugging failures

ğŸ“ src/

Purpose: All application logic (NO data here)

This is the heart of the system.

ğŸ“ src/ingestion01/

Purpose: Step 1 â€“ Data ingestion & preprocessing

preprocessing.py

Reads raw data from data/raw/

Performs:

Null handling

Date parsing

Type correction

Basic validations

Saves cleaned output to data/processed/

âŒ No feature engineering

âŒ No modeling logic

ğŸ“ src/features02/

Purpose: Step 2 â€“ Feature engineering

feature_engineering.py

Creates:

Lag features

Rolling means

Calendar joins

Demand classification (Low/Medium/High)

Fully driven by config.yaml

Outputs enhanced dataset to data/processed/

ğŸ“ src/models03/

Purpose: Step 3 â€“ Model development

model_trainer.py

Contains training logic for:

Linear Regression

Random Forest

XGBoost

LightGBM

Prophet

Ensemble models

Uses parameters from config

âŒ No hyperparameter tuning logic here

model_registry.py

Saves trained models

Loads models for inference

Handles versioning

ğŸ“ src/inventory04/

Purpose: Step 4 â€“ Inventory decision logic

inventory_planning.py

Converts demand forecasts into:

Safety stock

Reorder point

Uses business rules like CSL

This is business intelligence, not ML

ğŸ“ src/llm05/

Purpose: Step 5 â€“ LLM-based intelligence

llm_agent.py

Enables what-if analysis:

â€œWhat if demand increases by 20%?â€

â€œWhat if lead time doubles?â€

Uses LLM + regex fallback

Keeps system safe and explainable

ğŸ“ src/pipelines/

Purpose: Orchestration layer

run_pipeline.py

Runs end-to-end pipeline

Controls execution order

Used for automation and scheduling

ğŸ“ src/utils/

Purpose: Shared infrastructure code

config_loader.py

Loads and validates config.yaml

Fail-fast safety

logger.py

Centralized logging setup

Used by all modules

ğŸ“ tests/

Purpose: Quality assurance

Unit tests

Prevents regression

Professional standard

ğŸ“„ main.py

Single entry point

Calls pipeline logic

Used for execution

ğŸ“„ README.md

Complete project documentation

Explains architecture, flow, and usage

Mandatory for professional repos

(ids_env) PS C:\Users\KarthigeyanS\Documents\intelligent-demand-sensing> tree /f         
Folder PATH listing for volume Windows
Volume serial number is 962D-2395
C:.
â”‚   .gitignore
â”‚   main.py
â”‚   README.md
â”‚   reference.txt
â”‚   requirements.txt
â”‚   version_check.py
â”‚   
â”œâ”€â”€â”€config
â”‚       config.yaml
â”‚       __init__.py
â”‚
â”œâ”€â”€â”€data
â”‚   â”œâ”€â”€â”€external
â”‚   â”‚       assumptions.md
â”‚   â”‚
â”‚   â”œâ”€â”€â”€processed
â”‚   â”‚       cleaned_sales.parquet
â”‚   â”‚       features_sales.parquet
â”‚   â”‚       model_ready.parquet
â”‚   â”‚
â”‚   â””â”€â”€â”€raw
â”œâ”€â”€â”€logs
â”‚       ids_project.log
â”‚
â”œâ”€â”€â”€notebooks
â”‚       exploration.ipynb
â”‚
â”œâ”€â”€â”€output
â”‚   â”œâ”€â”€â”€forecasts
â”‚   â”œâ”€â”€â”€inventory
â”‚   â”œâ”€â”€â”€metrics
â”‚   â”œâ”€â”€â”€model
â”‚   â”œâ”€â”€â”€plots
â”‚   â”œâ”€â”€â”€reports
â”‚   â”‚       business_insights.md
â”‚   â”‚
â”‚   â””â”€â”€â”€visuals
â”œâ”€â”€â”€src
â”‚   â”‚   __init__.py
â”‚   â”‚   
â”‚   â”œâ”€â”€â”€features_02
â”‚   â”‚       calendar_features.py
â”‚   â”‚       feature_engineering.py
â”‚   â”‚       preprocess.py
â”‚   â”‚       __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€â”€ingestion_01
â”‚   â”‚       load_data.py
â”‚   â”‚       validate_schema.py
â”‚   â”‚       __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€â”€inventory04
â”‚   â”‚       inventory_planning.py
â”‚   â”‚       safety_stock.py
â”‚   â”‚       __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€â”€llm05
â”‚   â”‚       prompt_templates.py
â”‚   â”‚       what_if_agent.py
â”‚   â”‚       __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€â”€modeling_03
â”‚   â”‚       base_models.py
â”‚   â”‚       demand_classifier.py
â”‚   â”‚       evaluate.py
â”‚   â”‚       hyperparameter_tuning.py
â”‚   â”‚       stacking_ensemble.py
â”‚   â”‚       __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€â”€pipelines
â”‚   â”‚       run_features.py
â”‚   â”‚       run_fullpipeline.py
â”‚   â”‚       run_ingestion.py
â”‚   â”‚       run_inventory.py
â”‚   â”‚       run_training.py
â”‚   â”‚
â”‚   â”œâ”€â”€â”€utils
â”‚   â”‚       config_loader.py
â”‚   â”‚       helpers.py
â”‚   â”‚       logger.py
â”‚   â”‚       __init__.py
â”‚   â”‚
â”‚   â””â”€â”€â”€visualization06
â”‚           forecast_plots.py
â”‚           inventory_plots.py
â”‚           performance_plots.py
â”‚           __init__.py
â”‚
â””â”€â”€â”€tests

Coding Philosophy (NEW & IMPORTANT)

From now on, all code we write must be:

General â€“ reusable across projects

Config-driven â€“ no hardcoding

Defensive â€“ validates inputs and fails clearly

Explainable â€“ readable by humans and AI copilots

Portable â€“ minimal assumptions about data schema

Iâ€™ve saved this as a permanent rule for the IDS project.

Design & implement the FIRST reusable function:
Read raw sales data

Normalize column names

Apply rename map

Validate schema

Log clearly

Fail gracefully

This will become a template you can reuse in any future project.


This conversation is a continuation of my long-running project:

Project Name:
Intelligent Demand Sensing & Autonomous Inventory Planning (IDS)

Instruction (Important):
You must fully recall and strictly follow ALL prior discussions, decisions, and rules related to this project.
Do NOT treat this as a new project or a fresh start.

Continuity Rules:
1. Architecture is FINAL and FROZEN.
   - Repo structure with numbered modules (ingestion_01, features_02, modeling_03, inventory04, llm05, visualization06, pipelines, utils).
   - Config-driven design using config/config.yaml.
   - Centralized config_loader.py.
   - Strict separation of src/, data/raw, data/processed, output/.
   - No architectural changes unless I explicitly ask and approve them.

2. Coding Standards are FINAL and MUST be enforced:
   - All code must be general, reusable, and portable to other projects wherever possible.
   - Functions must be config-driven, not hardcoded.
   - Column names must be normalized to snake_case (auto convert, replace spaces, '/', special chars).
   - Use flexible rename maps to handle multiple real-world column variants.
   - Defensive programming: validate inputs, schemas, and configs early.
   - Clear, actionable error messages that explain:
        â€¢ what is missing
        â€¢ what was found
        â€¢ how the user can fix it (e.g., extend rename_map)
   - Separate I/O from business logic.
   - Use type hints, docstrings, logging (not print), and meaningful function names.

3. Teaching Style (Mandatory):
   - Treat me as a fresher-to-intermediate engineer being trained in INDUSTRIAL practices.
   - Before coding:
        â€¢ Explain WHY we are doing something
        â€¢ Explain WHAT problem it solves
        â€¢ Explain WHEN it can be reused in other projects
   - Clearly define every function:
        â€¢ Purpose
        â€¢ Parameters
        â€¢ Expected inputs/outputs
        â€¢ Reusability notes
   - If something cannot be made generic, explain WHY and keep it project-specific.

4. Workflow Discipline:
   - Do NOT repeat already completed steps.
   - Do NOT jump ahead.
   - Confirm the current phase before proceeding.
   - Always align with the existing repo state and last agreed checkpoint.

5. Scope Reminder:
   - This project includes:
        â€¢ Data ingestion & validation
        â€¢ Feature engineering with calendar/festival joins
        â€¢ Demand classification (Low/Medium/High)
        â€¢ Multiple forecasting models + stacking ensemble
        â€¢ Inventory planning & safety stock (95% CSL)
        â€¢ LLM-based what-if agent (with regex fallback)
        â€¢ Production-style pipelines & logging

6. Strictness Clause:
   - Be strict.
   - Challenge bad practices.
   - Prefer long-term maintainability over shortcuts.
   - Think like a Solution Architect / Tech Lead guiding a production system.

Start by:
- Briefly summarizing where we left off
- Stating the NEXT logical step
- Explaining the industrial reasoning before writing any code




QUICK â€œLITEâ€ PROMPT
Continue my ongoing AI project from the last checkpoint.

Rules:
- Recall all prior architecture, decisions, and completed steps.
- Architecture and coding standards are frozen unless I approve changes.
- Code must be reusable, config-driven, defensive, and industry-grade.
- Do not repeat or restart completed work.

Start by summarizing where we left off and propose the next step.
